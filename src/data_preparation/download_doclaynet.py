"""
DocLayNet ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

Hugging Faceì—ì„œ DocLayNet ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  COCO í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
- 69,375 train ìƒ˜í”Œ
- 6,489 validation ìƒ˜í”Œ
- 4,999 test ìƒ˜í”Œ
- 11ê°œ ë ˆì´ì•„ì›ƒ í´ë˜ìŠ¤

ì‚¬ìš©ë²•:
    python src/data_preparation/download_doclaynet.py --output data/doclaynet

ì£¼ì˜:
    datasets==2.14.5 ë²„ì „ í•„ìš” (loading script ì§€ì›)
"""

import argparse
import json
from pathlib import Path
from datasets import load_dataset
from tqdm import tqdm


def download_doclaynet(output_dir: str):
    """
    DocLayNet ë°ì´í„°ì…‹ì„ Hugging Faceì—ì„œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        output_dir: ë‹¤ìš´ë¡œë“œí•  ë””ë ‰í† ë¦¬ ê²½ë¡œ

    DocLayNet í´ë˜ìŠ¤ (11ê°œ):
        0: Caption - ì´ë¯¸ì§€/í‘œ ìº¡ì…˜
        1: Footnote - ê°ì£¼
        2: Formula - ìˆ˜ì‹
        3: List-item - ë¦¬ìŠ¤íŠ¸ í•­ëª©
        4: Page-footer - í˜ì´ì§€ í•˜ë‹¨
        5: Page-header - í˜ì´ì§€ ìƒë‹¨
        6: Picture - ì´ë¯¸ì§€/ê·¸ë¦¼
        7: Section-header - ì„¹ì…˜ í—¤ë”
        8: Table - í‘œ
        9: Text - ë³¸ë¬¸ í…ìŠ¤íŠ¸
        10: Title - ì œëª©

    ë°ì´í„° êµ¬ì¡°:
        - image_id: ì´ë¯¸ì§€ ê³ ìœ  ID
        - image: PIL ì´ë¯¸ì§€ ê°ì²´
        - width, height: ì´ë¯¸ì§€ í¬ê¸°
        - doc_category: ë¬¸ì„œ ì¹´í…Œê³ ë¦¬
        - objects: ë°”ìš´ë”© ë°•ìŠ¤ ì–´ë…¸í…Œì´ì…˜
            - bbox: [x_min, y_min, width, height]
            - category_id: í´ë˜ìŠ¤ ID (0-10)
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    print("=" * 70)
    print("DocLayNet ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ")
    print("=" * 70)
    print(f"ì €ì¥ ê²½ë¡œ: {output_path}")
    print("=" * 70 + "\n")

    try:
        print("Hugging Faceì—ì„œ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")

        # Hugging Faceì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ (datasets==2.14.5 í•„ìš”)
        dataset = load_dataset("ds4sd/DocLayNet")

        print(f"\në°ì´í„°ì…‹ ì •ë³´:")
        print(f"  - Train: {len(dataset['train']):,} ìƒ˜í”Œ")
        print(f"  - Validation: {len(dataset['validation']):,} ìƒ˜í”Œ")
        print(f"  - Test: {len(dataset['test']):,} ìƒ˜í”Œ")
        print(f"  - ì´ {len(dataset['train']) + len(dataset['validation']) + len(dataset['test']):,} ìƒ˜í”Œ")

        # ê° ë¶„í• (split)ì„ ì €ì¥
        for split_name in ['train', 'validation', 'test']:
            split_dir = output_path / split_name
            split_dir.mkdir(parents=True, exist_ok=True)

            images_dir = split_dir / "images"
            images_dir.mkdir(exist_ok=True)

            print(f"\n{'='*70}")
            print(f"{split_name.upper()} ë°ì´í„° ì €ì¥ ì¤‘...")
            print(f"{'='*70}")

            split_data = dataset[split_name]

            # COCO í˜•ì‹ ì–´ë…¸í…Œì´ì…˜ ì¤€ë¹„
            coco_annotations = {
                "images": [],
                "annotations": [],
                "categories": [
                    {"id": 0, "name": "Caption"},
                    {"id": 1, "name": "Footnote"},
                    {"id": 2, "name": "Formula"},
                    {"id": 3, "name": "List-item"},
                    {"id": 4, "name": "Page-footer"},
                    {"id": 5, "name": "Page-header"},
                    {"id": 6, "name": "Picture"},
                    {"id": 7, "name": "Section-header"},
                    {"id": 8, "name": "Table"},
                    {"id": 9, "name": "Text"},
                    {"id": 10, "name": "Title"}
                ]
            }

            ann_id = 0

            # ê° ìƒ˜í”Œ ì²˜ë¦¬
            for idx, sample in enumerate(tqdm(split_data, desc=f"Processing {split_name}")):
                # ì´ë¯¸ì§€ ì €ì¥
                image = sample['image']
                image_filename = f"{sample['image_id']}.png"
                image_path = images_dir / image_filename

                # PIL ì´ë¯¸ì§€ë¥¼ PNGë¡œ ì €ì¥
                image.save(image_path)

                # COCO ì´ë¯¸ì§€ ì •ë³´
                coco_annotations["images"].append({
                    "id": idx,
                    "file_name": image_filename,
                    "width": sample['width'],
                    "height": sample['height'],
                    "doc_category": sample.get('doc_category', ''),
                    "image_id": sample['image_id']
                })

                # COCO ì–´ë…¸í…Œì´ì…˜ ì •ë³´
                # objectsëŠ” ì—¬ëŸ¬ ê°œì˜ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ í¬í•¨
                if 'objects' in sample and sample['objects'] is not None:
                    objects = sample['objects']
                    
                    # dict í˜•íƒœ: {'bbox': [...], 'category_id': [...]}
                    if isinstance(objects, dict):
                        bboxes = objects.get('bbox', [])
                        category_ids = objects.get('category_id', [])
                        
                        for bbox, category_id in zip(bboxes, category_ids):
                            # bbox: [x_min, y_min, width, height] (COCO í˜•ì‹)
                            coco_annotations["annotations"].append({
                                "id": ann_id,
                                "image_id": idx,
                                "category_id": category_id,
                                "bbox": bbox,
                                "area": bbox[2] * bbox[3],  # width * height
                                "iscrowd": 0
                            })
                            ann_id += 1
                    
                    # list í˜•íƒœ: [{'bbox': [...], 'category_id': ...}, ...]
                    elif isinstance(objects, list):
                        for obj in objects:
                            bbox = obj.get('bbox', obj.get('bboxes', []))
                            category_id = obj.get('category_id', obj.get('category', 0))
                            
                            if bbox:
                                coco_annotations["annotations"].append({
                                    "id": ann_id,
                                    "image_id": idx,
                                    "category_id": category_id,
                                    "bbox": bbox,
                                    "area": bbox[2] * bbox[3],  # width * height
                                    "iscrowd": 0
                                })
                                ann_id += 1

            # COCO JSON ì €ì¥
            json_path = split_dir / "annotations.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(coco_annotations, f, indent=2)

            print(f"\nâœ… {split_name} ì €ì¥ ì™„ë£Œ:")
            print(f"   - ì´ë¯¸ì§€: {len(coco_annotations['images']):,}ê°œ")
            print(f"   - ì–´ë…¸í…Œì´ì…˜: {ann_id:,}ê°œ")
            print(f"   - JSON: {json_path}")

        print("\n" + "=" * 70)
        print("âœ… DocLayNet ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        print("=" * 70)

        # ë°ì´í„°ì…‹ êµ¬ì¡° ì•ˆë‚´
        print("\nğŸ“ ë°ì´í„°ì…‹ êµ¬ì¡°:")
        print(f"{output_path}/")
        print("  â”œâ”€â”€ train/")
        print("  â”‚   â”œâ”€â”€ images/")
        print("  â”‚   â”‚   â””â”€â”€ *.png")
        print("  â”‚   â””â”€â”€ annotations.json")
        print("  â”œâ”€â”€ validation/")
        print("  â”‚   â”œâ”€â”€ images/")
        print("  â”‚   â”‚   â””â”€â”€ *.png")
        print("  â”‚   â””â”€â”€ annotations.json")
        print("  â””â”€â”€ test/")
        print("      â”œâ”€â”€ images/")
        print("      â”‚   â””â”€â”€ *.png")
        print("      â””â”€â”€ annotations.json")

        print("\në‹¤ìŒ ë‹¨ê³„: COCO â†’ YOLO í˜•ì‹ ë³€í™˜")
        print(f"  python src/data_preparation/convert_to_yolo.py \\")
        print(f"    --input {output_path} \\")
        print(f"    --output {output_path / 'yolo'}")

    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        print("\në¬¸ì œ í•´ê²°:")
        print("1. datasets ë²„ì „ í™•ì¸:")
        print("   uv pip install 'datasets==2.14.5'")
        print("2. ì¸í„°ë„· ì—°ê²° í™•ì¸")
        print("3. ì¶©ë¶„í•œ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (ì•½ 30GB í•„ìš”)")
        raise


def main():
    parser = argparse.ArgumentParser(
        description="DocLayNet ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="data/doclaynet",
        help="ë‹¤ìš´ë¡œë“œí•  ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ê°’: data/doclaynet)"
    )

    args = parser.parse_args()

    print("\nğŸ“Š DocLayNet ë°ì´í„°ì…‹ ì •ë³´:")
    print("  - ì¶œì²˜: IBM Research")
    print("  - ì´ ìƒ˜í”Œ: ~80,000 í˜ì´ì§€")
    print("  - í´ë˜ìŠ¤: 11ê°œ ë ˆì´ì•„ì›ƒ ìš”ì†Œ")
    print("  - ì´ë¯¸ì§€ í¬ê¸°: 1025x1025 px")
    print("  - í˜•ì‹: PNG + COCO JSON")
    print("\nâš ï¸ ì²˜ìŒ ë‹¤ìš´ë¡œë“œ ì‹œ ~30GBì˜ ë°ì´í„°ê°€ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤.")
    print("âš ï¸ datasets==2.14.5 ë²„ì „ í•„ìš” (loading script ì§€ì›)")

    download_doclaynet(args.output)


if __name__ == "__main__":
    main()
