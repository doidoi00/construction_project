"""
í•™ìŠµëœ YOLO ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê°€í•˜ê³ ,
í´ë˜ìŠ¤ë³„ ìƒì„¸ ì„±ëŠ¥ ë¶„ì„ ë° ì‹œê°í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    # ê¸°ë³¸ í‰ê°€
    python src/training/evaluate.py \
        --model models/finetuned/doclaynet_yolo11n/weights/best.pt \
        --data data/doclaynet/yolo/doclaynet.yaml

    # íŠ¹ì • ë¶„í•  í‰ê°€
    python src/training/evaluate.py \
        --model models/finetuned/best.pt \
        --data data/doclaynet/yolo/doclaynet.yaml \
        --split test
"""

import argparse
from pathlib import Path
import json
import matplotlib.pyplot as plt
import seaborn as sns
from ultralytics import YOLO


def evaluate_model(model_path: str, data_path: str, split: str = "val", save_dir: str = None):
    """
    ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

    Args:
        model_path: í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (.pt íŒŒì¼)
        data_path: ë°ì´í„°ì…‹ YAML íŒŒì¼ ê²½ë¡œ
        split: í‰ê°€í•  ë°ì´í„° ë¶„í•  (val, test)
        save_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬

    í‰ê°€ ì§€í‘œ:
        - mAP@0.5: IoU 0.5ì—ì„œì˜ mean Average Precision
        - mAP@0.5:0.95: IoU 0.5~0.95ì—ì„œì˜ mAP
        - Precision: ì •ë°€ë„
        - Recall: ì¬í˜„ìœ¨
        - F1 Score: F1 ì ìˆ˜

    ì¶œë ¥:
        - ì „ì²´ ì„±ëŠ¥ ì§€í‘œ
        - í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„
        - í˜¼ë™ í–‰ë ¬ (Confusion Matrix)
        - ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
    """
    print("=" * 70)
    print("ëª¨ë¸ í‰ê°€")
    print("=" * 70)
    print(f"ëª¨ë¸: {model_path}")
    print(f"ë°ì´í„°: {data_path}")
    print(f"ë¶„í• : {split}")
    print("=" * 70 + "\n")

    # ëª¨ë¸ ë¡œë“œ
    model = YOLO(model_path)

    # í‰ê°€ ì‹¤í–‰
    print("í‰ê°€ ì‹œì‘...\n")
    results = model.val(
        data=data_path,
        split=split,
        save_json=True,  # COCO JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ì €ì¥
        save_hybrid=True,  # ë¼ë²¨ê³¼ ì˜ˆì¸¡ í•¨ê»˜ ì €ì¥
        plots=True  # ì‹œê°í™” ìƒì„±
    )

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 70)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼")
    print("=" * 70)

    # ì „ì²´ ì„±ëŠ¥ ì§€í‘œ
    print("\nì „ì²´ ì„±ëŠ¥:")
    print(f"  mAP@0.5:      {results.box.map50:.4f}")
    print(f"  mAP@0.5:0.95: {results.box.map:.4f}")
    print(f"  Precision:    {results.box.mp:.4f}")
    print(f"  Recall:       {results.box.mr:.4f}")

    # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ (ìƒìœ„ 3ê°œ í´ë˜ìŠ¤)
    print("\ní´ë˜ìŠ¤ë³„ ì„±ëŠ¥ (mAP@0.5):")

    # DocLayNet í´ë˜ìŠ¤ ì´ë¦„
    class_names = [
        "Caption", "Footnote", "Formula", "List-item",
        "Page-footer", "Page-header", "Picture", "Section-header",
        "Table", "Text", "Title"
    ]

    # í´ë˜ìŠ¤ë³„ AP ì¶œë ¥
    if hasattr(results.box, 'ap_class_index') and hasattr(results.box, 'ap50'):
        class_ap = {}
        for idx, ap in zip(results.box.ap_class_index, results.box.ap50):
            class_name = class_names[idx] if idx < len(class_names) else f"Class {idx}"
            class_ap[class_name] = ap
            print(f"  {class_name:15s}: {ap:.4f}")

        # Table í´ë˜ìŠ¤ íŠ¹í™” ë¶„ì„
        if "Table" in class_ap:
            print(f"\nâ­ Table í´ë˜ìŠ¤ ì„±ëŠ¥: {class_ap['Table']:.4f}")
            if class_ap['Table'] > 0.9:
                print("   âœ… ìš°ìˆ˜í•œ í‘œ ê°ì§€ ì„±ëŠ¥!")
            elif class_ap['Table'] > 0.7:
                print("   âœ… ì–‘í˜¸í•œ í‘œ ê°ì§€ ì„±ëŠ¥")
            else:
                print("   âš ï¸ í‘œ ê°ì§€ ì„±ëŠ¥ ê°œì„  í•„ìš”")

    print("\n" + "=" * 70)

    # ê²°ê³¼ ì €ì¥
    if save_dir:
        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)

        # JSONìœ¼ë¡œ ê²°ê³¼ ì €ì¥
        results_dict = {
            "model": str(model_path),
            "data": str(data_path),
            "split": split,
            "metrics": {
                "mAP@0.5": float(results.box.map50),
                "mAP@0.5:0.95": float(results.box.map),
                "precision": float(results.box.mp),
                "recall": float(results.box.mr)
            }
        }

        if hasattr(results.box, 'ap_class_index') and hasattr(results.box, 'ap50'):
            results_dict["class_metrics"] = {}
            for idx, ap in zip(results.box.ap_class_index, results.box.ap50):
                class_name = class_names[idx] if idx < len(class_names) else f"Class {idx}"
                results_dict["class_metrics"][class_name] = float(ap)

        json_path = save_path / "evaluation_results.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, indent=2, ensure_ascii=False)

        print(f"\nâœ… ê²°ê³¼ ì €ì¥: {json_path}")

    return results


def compare_models(model_paths: list, data_path: str, split: str = "val"):
    """
    ì—¬ëŸ¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.

    Args:
        model_paths: ë¹„êµí•  ëª¨ë¸ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        data_path: ë°ì´í„°ì…‹ YAML ê²½ë¡œ
        split: í‰ê°€ ë¶„í• 

    ì¶œë ¥:
        ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ í‘œ
        ì‹œê°í™” ì°¨íŠ¸
    """
    print("=" * 70)
    print("ëª¨ë¸ ë¹„êµ")
    print("=" * 70)

    results_list = []

    for model_path in model_paths:
        print(f"\ní‰ê°€ ì¤‘: {model_path}")
        model = YOLO(model_path)
        results = model.val(data=data_path, split=split, verbose=False)

        results_list.append({
            "name": Path(model_path).stem,
            "mAP@0.5": results.box.map50,
            "mAP@0.5:0.95": results.box.map,
            "precision": results.box.mp,
            "recall": results.box.mr
        })

    # ë¹„êµ í‘œ ì¶œë ¥
    print("\n" + "=" * 70)
    print("ğŸ“Š ëª¨ë¸ ë¹„êµ ê²°ê³¼")
    print("=" * 70)
    print(f"{'ëª¨ë¸':<30s} {'mAP@0.5':<12s} {'mAP@0.5:0.95':<15s} {'Precision':<12s} {'Recall':<12s}")
    print("-" * 70)

    for r in results_list:
        print(f"{r['name']:<30s} {r['mAP@0.5']:<12.4f} {r['mAP@0.5:0.95']:<15.4f} {r['precision']:<12.4f} {r['recall']:<12.4f}")

    print("=" * 70)

    # ì‹œê°í™”
    try:
        import numpy as np

        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle('Model Performance Comparison', fontsize=16)

        metrics = ['mAP@0.5', 'mAP@0.5:0.95', 'precision', 'recall']
        titles = ['mAP@0.5', 'mAP@0.5:0.95', 'Precision', 'Recall']

        for idx, (metric, title) in enumerate(zip(metrics, titles)):
            ax = axes[idx // 2, idx % 2]
            names = [r['name'] for r in results_list]
            values = [r[metric] for r in results_list]

            ax.bar(names, values)
            ax.set_ylabel(title)
            ax.set_ylim([0, 1])
            ax.grid(axis='y', alpha=0.3)

        plt.tight_layout()
        plt.savefig('model_comparison.png', dpi=300)
        print(f"\nâœ… ë¹„êµ ì°¨íŠ¸ ì €ì¥: model_comparison.png")

    except Exception as e:
        print(f"\nâš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")


def main():
    parser = argparse.ArgumentParser(
        description="YOLO ëª¨ë¸ í‰ê°€ ë° ë¶„ì„"
    )
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        help="í‰ê°€í•  ëª¨ë¸ ê²½ë¡œ (.pt íŒŒì¼)"
    )
    parser.add_argument(
        "--data",
        type=str,
        required=True,
        help="ë°ì´í„°ì…‹ YAML íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--split",
        type=str,
        default="val",
        choices=["val", "test"],
        help="í‰ê°€í•  ë°ì´í„° ë¶„í•  (ê¸°ë³¸ê°’: val)"
    )
    parser.add_argument(
        "--save-dir",
        type=str,
        help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--compare",
        nargs='+',
        help="ë¹„êµí•  ì¶”ê°€ ëª¨ë¸ ê²½ë¡œë“¤"
    )

    args = parser.parse_args()

    if args.compare:
        # ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ
        all_models = [args.model] + args.compare
        compare_models(all_models, args.data, args.split)
    else:
        # ë‹¨ì¼ ëª¨ë¸ í‰ê°€
        evaluate_model(args.model, args.data, args.split, args.save_dir)


if __name__ == "__main__":
    main()
